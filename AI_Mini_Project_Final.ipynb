{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdc_9IBMCal-"
   },
   "source": [
    "# Training The Model For Music Recommender System\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yWQPM3pxC0bZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from the Spotify Millsong dataset.\n",
    "songs_dataset = pd.read_csv(r\"D:\\Studies\\Semester 7\\AI\\AI\\spotify_millsongdata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'link' column from the dataset as it is not needed for our operations.\n",
    "new_songs_dataset = songs_dataset.drop('link', axis=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P_FnMUw58jz4"
   },
   "outputs": [],
   "source": [
    "# Provide replacement for unwanted characters with a single space.\n",
    "new_songs_dataset['text'] = (\n",
    "    new_songs_dataset['text']\n",
    "    .str.lower()\n",
    "    .replace(r'[^a-z0-9\\s]', ' ', regex=True)\n",
    "    .replace(r'\\n', ' ', regex=True)\n",
    "    .replace(r'\\r', ' ', regex=True)\n",
    "    .replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ladOQBYtI69B"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ahsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenization(txt):\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    stemming = [stemmer.stem(w) for w in tokens]\n",
    "    return \" \".join(stemming)\n",
    "\n",
    "# Apply tokenization into dataset.\n",
    "new_songs_dataset['text'] = new_songs_dataset['text'].apply(lambda x: tokenization(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WlEaw6EUJBBj"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the first 20000 songs as the dataset.\n",
    "first_20000_songs_dataset = new_songs_dataset.head(5000)\n",
    "\n",
    "# Transfer the tokenized text into a vector and find the minimum angular distance.\n",
    "vector = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "matrix = vector.fit_transform(first_20000_songs_dataset['text'])\n",
    "distance_similarity = cosine_similarity(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_y-8oppRJwIK"
   },
   "outputs": [],
   "source": [
    "def recommendation(lyrics_input):\n",
    "    # Preprocess the input lyrics\n",
    "    lyrics_input = tokenization(lyrics_input)\n",
    "\n",
    "    # Calculate similarity between input lyrics and dataset\n",
    "    similarity_with_input = cosine_similarity(vector.transform([lyrics_input]), matrix).flatten()\n",
    "\n",
    "    # Get the indices of songs sorted by similarity\n",
    "    indices_sorted_by_similarity = similarity_with_input.argsort()[::-1]\n",
    "\n",
    "    recommended_songs = []\n",
    "    for i in indices_sorted_by_similarity[1:6]:\n",
    "        recommended_songs.append(first_20000_songs_dataset.iloc[i].song)\n",
    "\n",
    "    return recommended_songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZrFneDduKYTn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Songs: ['Be Kind To Me', 'I Am Just A Girl', \"That's Me\", 'Kimono Girl', 'Famous Girl']\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "recommended_songs = recommendation(\"She's just my kind of girl, she makes me feel fine Who could ever believe that she could be mine?  She's just my kind of girl, without her I'm blue  And if she ever leaves me what could I do, what could I do?  \")\n",
    "print(\"Recommended Songs:\", recommended_songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (existing code)\n",
    "\n",
    "# Transfer the tokenized text into a vector and find the minimum angular distance.\n",
    "vector = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "matrix = vector.fit_transform(first_20000_songs_dataset['text'])\n",
    "distance_similarity = cosine_similarity(matrix)\n",
    "\n",
    "# Dump necessary data to pickle files\n",
    "import pickle\n",
    "pickle.dump(distance_similarity, open('distance_similarity.pkl', 'wb'))\n",
    "pickle.dump(first_20000_songs_dataset, open('first_20000_songs_dataset.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
